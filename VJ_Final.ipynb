{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the positive and negative folders\n",
    "dataset_path = os.path.abspath(\"Widerfaces2\")  \n",
    "positive_path = os.path.join(dataset_path, \"Positive\")\n",
    "negative_path = os.path.join(dataset_path, \"Negative\")\n",
    "\n",
    "# Verify paths\n",
    "if not os.path.exists(positive_path):\n",
    "    print(f\"Error: Positive path '{positive_path}' does not exist!\")\n",
    "if not os.path.exists(negative_path):\n",
    "    print(f\"Error: Negative path '{negative_path}' does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 2330\n",
      "Labels distribution: (array([0, 1]), array([ 150, 2180], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_images(folder_path, label, size=(24, 24)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(('.jpg', '.png')):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue  # Skip unreadable images\n",
    "            img_resized = cv2.resize(img, size)\n",
    "            images.append(img_resized)\n",
    "            labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Preprocess Positive and Negative samples\n",
    "positive_images, positive_labels = preprocess_images(positive_path, label=1)\n",
    "negative_images, negative_labels = preprocess_images(negative_path, label=0)\n",
    "\n",
    "# Combine datasets\n",
    "all_images = np.vstack((positive_images, negative_images))\n",
    "all_labels = np.hstack((positive_labels, negative_labels))\n",
    "\n",
    "print(f\"Total images: {len(all_images)}\")\n",
    "print(f\"Labels distribution: {np.unique(all_labels, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Haar features for 2330 images.\n",
      "Training samples: 1864, Testing samples: 466\n"
     ]
    }
   ],
   "source": [
    "# Compute integral image\n",
    "def compute_integral_image(image):\n",
    "    integral_img = np.zeros_like(image, dtype=np.int32)\n",
    "    for i in range(image.shape[0]):\n",
    "        for j in range(image.shape[1]):\n",
    "            integral_img[i, j] = image[i, j] + \\\n",
    "                                 (integral_img[i-1, j] if i > 0 else 0) + \\\n",
    "                                 (integral_img[i, j-1] if j > 0 else 0) - \\\n",
    "                                 (integral_img[i-1, j-1] if i > 0 and j > 0 else 0)\n",
    "    return integral_img\n",
    "\n",
    "# Haar-like features\n",
    "def haar_feature_sum(integral_image, x, y, width, height, feature_type):\n",
    "    if feature_type == 'edge':\n",
    "        mid = height // 2\n",
    "        top_sum = integral_image[y + mid, x + width] - integral_image[y, x + width] - integral_image[y + mid, x] + integral_image[y, x]\n",
    "        bottom_sum = integral_image[y + height, x + width] - integral_image[y + mid, x + width] - integral_image[y + height, x] + integral_image[y + mid, x]\n",
    "        return bottom_sum - top_sum\n",
    "    elif feature_type == 'line':\n",
    "        mid = width // 2\n",
    "        left_sum = integral_image[y + height, x + mid] - integral_image[y, x + mid] - integral_image[y + height, x] + integral_image[y, x]\n",
    "        right_sum = integral_image[y + height, x + width] - integral_image[y, x + width] - integral_image[y + height, x + mid] + integral_image[y, x + mid]\n",
    "        return right_sum - left_sum\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature type\")\n",
    "\n",
    "# Extract Haar features\n",
    "def extract_haar_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        integral_img = compute_integral_image(img)\n",
    "        feature_vec = []\n",
    "        for x in range(0, img.shape[1] - 10, 10):\n",
    "            for y in range(0, img.shape[0] - 10, 10):\n",
    "                feature_vec.append(haar_feature_sum(integral_img, x, y, 10, 10, 'edge'))\n",
    "        features.append(feature_vec)\n",
    "    return np.array(features)\n",
    "\n",
    "# Extract features\n",
    "haar_features = extract_haar_features(all_images)\n",
    "print(f\"Extracted Haar features for {len(haar_features)} images.\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(haar_features, all_labels, test_size=0.2, random_state=42)\n",
    "print(f\"Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
    "\n",
    "# Decision Stump class\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.feature_idx = None  \n",
    "        self.threshold = None  \n",
    "        self.polarity = 1  \n",
    "        self.alpha = 0 \n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = np.ones(n_samples)\n",
    "        if self.polarity == 1:\n",
    "            predictions[X[:, self.feature_idx] < self.threshold] = -1\n",
    "        else:\n",
    "            predictions[X[:, self.feature_idx] >= self.threshold] = -1\n",
    "        return predictions\n",
    "\n",
    "# Train decision stump\n",
    "def train_decision_stump(X, y, sample_weights):\n",
    "    n_samples, n_features = X.shape\n",
    "    stump = DecisionStump()\n",
    "    min_error = float('inf')\n",
    "\n",
    "    for feature_idx in range(n_features):\n",
    "        feature_values = X[:, feature_idx]\n",
    "        thresholds = np.unique(feature_values)\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            for polarity in [1, -1]:\n",
    "                predictions = np.ones(n_samples)\n",
    "                if polarity == 1:\n",
    "                    predictions[feature_values < threshold] = -1\n",
    "                else:\n",
    "                    predictions[feature_values >= threshold] = -1\n",
    "\n",
    "                error = np.sum(sample_weights[y != predictions])\n",
    "                if error < min_error:\n",
    "                    min_error = error\n",
    "                    stump.feature_idx = feature_idx\n",
    "                    stump.threshold = threshold\n",
    "                    stump.polarity = polarity\n",
    "\n",
    "    return stump, min_error\n",
    "\n",
    "# Train AdaBoost\n",
    "def train_adaboost(X, y, n_classifiers):\n",
    "    n_samples, n_features = X.shape\n",
    "    sample_weights = np.ones(n_samples) / n_samples\n",
    "    classifiers = []\n",
    "\n",
    "    for _ in range(n_classifiers):\n",
    "        stump, error = train_decision_stump(X, y, sample_weights)\n",
    "        epsilon = 1e-10\n",
    "        stump.alpha = 0.5 * np.log((1 - error) / (error + epsilon))\n",
    "        predictions = stump.predict(X)\n",
    "        sample_weights *= np.exp(-stump.alpha * y * predictions)\n",
    "        sample_weights /= np.sum(sample_weights)\n",
    "        classifiers.append(stump)\n",
    "\n",
    "    return classifiers\n",
    "\n",
    "# Predict with AdaBoost\n",
    "def adaboost_predict(classifiers, X):\n",
    "    n_samples = X.shape[0]\n",
    "    final_predictions = np.zeros(n_samples)\n",
    "\n",
    "    for stump in classifiers:\n",
    "        predictions = stump.predict(X)\n",
    "        final_predictions += stump.alpha * predictions\n",
    "\n",
    "    return np.sign(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained stage 1 with 5 features.\n",
      "Trained stage 2 with 10 features.\n",
      "Trained stage 3 with 20 features.\n"
     ]
    }
   ],
   "source": [
    "# Train cascaded classifiers\n",
    "def train_cascade(X, y, stage_features):\n",
    "    cascade_classifiers = []\n",
    "    for stage, num_features in enumerate(stage_features):\n",
    "        X_stage = X[:, :num_features]\n",
    "        classifiers = train_adaboost(X_stage, y, n_classifiers=100)\n",
    "        cascade_classifiers.append(classifiers)\n",
    "        print(f\"Trained stage {stage+1} with {num_features} features.\")\n",
    "    return cascade_classifiers\n",
    "\n",
    "# Predict with cascaded classifiers\n",
    "def cascade_predict(cascade_classifiers, X):\n",
    "    \"\"\"\n",
    "    Predict using cascaded classifiers with early rejection.\n",
    "    Args:\n",
    "        cascade_classifiers: List of classifiers for each stage.\n",
    "        X: Feature matrix for prediction.\n",
    "    Returns:\n",
    "        Predictions for the input data.\n",
    "    \"\"\"\n",
    "    for stage, classifiers in enumerate(cascade_classifiers):\n",
    "        \n",
    "        num_features = stage_features[stage]\n",
    "        X_stage = X[:, :num_features]\n",
    "        \n",
    "        \n",
    "        predictions = adaboost_predict(classifiers, X_stage)\n",
    "        \n",
    "        # Early rejection: If any sample is classified as negative, reject it\n",
    "        if np.any(predictions == -1):\n",
    "            return -1  \n",
    "    return 1  \n",
    "\n",
    "\n",
    "# Define cascade stages\n",
    "stage_features = [5, 10, 20]\n",
    "\n",
    "# Convert labels to {-1, 1}\n",
    "y_train_boost = np.where(y_train == 1, 1, -1)\n",
    "y_test_boost = np.where(y_test == 1, 1, -1)\n",
    "\n",
    "# Train the cascade\n",
    "cascade_classifiers = train_cascade(X_train, y_train_boost, stage_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cascade Test Accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate cascade\n",
    "cascade_results = [cascade_predict(cascade_classifiers, sample.reshape(1, -1)) for sample in X_test]\n",
    "cascade_accuracy = np.mean(np.array(cascade_results) == y_test_boost)\n",
    "print(f\"Cascade Test Accuracy: {cascade_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
